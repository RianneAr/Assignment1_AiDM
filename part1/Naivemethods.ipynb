{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31de8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "ratings = pd.read_table('ml-1m/ratings.dat',  sep = '::', engine = 'python', header = None, names= ['UserID', 'MovieID', 'Rating', 'Timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af380394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change invalid ratings to valid ones\n",
    "ratings['Rating'].values[ratings['Rating'].values > 5] = 5\n",
    "ratings['Rating'].values[ratings['Rating'].values < 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50bc20e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      1       2       3 ... 1000205 1000207 1000208] [      0       7      12 ... 1000196 1000198 1000206]\n",
      "[      0       1       2 ... 1000205 1000206 1000208] [      6       9      10 ... 1000192 1000200 1000207]\n",
      "[      0       1       3 ... 1000205 1000206 1000207] [      2      11      26 ... 1000201 1000204 1000208]\n",
      "[      0       2       3 ... 1000206 1000207 1000208] [      1       4      16 ... 1000175 1000193 1000199]\n",
      "[      0       1       2 ... 1000206 1000207 1000208] [      3       5       8 ... 1000202 1000203 1000205]\n"
     ]
    }
   ],
   "source": [
    "# Create k-fold indices\n",
    "kf = KFold(n_splits = 5, shuffle=True, random_state=random_seed)\n",
    "for train, test in kf.split(ratings):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14abcb3",
   "metadata": {},
   "source": [
    "First, we try out the three most naive methods: predicting the global average, the average of the user, or the average of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef713038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global rmse:  1.117101450528282\n",
      "global mae:  0.9338619186570455\n",
      "user rmse:  1.0354800404316435\n",
      "user mae:  0.8289498348484926\n",
      "item rmse:  0.9793666761448853\n",
      "item mae:  0.7822841972440417\n"
     ]
    }
   ],
   "source": [
    "rmseglobalavg = []\n",
    "maeglobalavg = []\n",
    "rmseuseravg = []\n",
    "maeuseravg = []\n",
    "rmseitemavg = []\n",
    "maeitemavg = []\n",
    "\n",
    "for trainid, testid in kf.split(ratings):\n",
    "    \n",
    "    X_train = ratings[['UserID','MovieID']].iloc[trainid]\n",
    "    X_test = ratings[['UserID','MovieID']].iloc[testid]\n",
    "    y_train = ratings['Rating'].iloc[trainid]\n",
    "    y_test = ratings['Rating'].iloc[testid]\n",
    "    \n",
    "    # define averages\n",
    "    global_avg = y_train.mean()\n",
    "    user_avg = ratings[['UserID','Rating']].iloc[trainid].groupby(['UserID']).mean()\n",
    "    item_avg = ratings[['MovieID','Rating']].iloc[trainid].groupby(['MovieID']).mean()\n",
    " \n",
    "    # create lists to put results in\n",
    "    global_avg_pred = []\n",
    "    user_avg_pred = []\n",
    "    item_avg_pred = []\n",
    "    \n",
    "    # loop over test set\n",
    "    for x in testid:\n",
    "       \n",
    "        # predict using global average\n",
    "        global_avg_pred.append(global_avg)\n",
    "    \n",
    "        # predict using user average\n",
    "        if ratings['UserID'][x] in user_avg.index.tolist():\n",
    "            user_rating = user_avg.loc[ratings['UserID'][x]]\n",
    "            user_avg_pred.append(user_rating[0])\n",
    "        else: user_avg_pred.append(global_avg)\n",
    "        \n",
    "        # predict using item average\n",
    "        if ratings['MovieID'][x] in item_avg.index.tolist():\n",
    "            item_rating = item_avg.loc[ratings['MovieID'][x]]\n",
    "            item_avg_pred.append(item_rating[0])\n",
    "        else: item_avg_pred.append(global_avg)\n",
    "\n",
    "    # append results to lists\n",
    "    rmseglobalavg.append(math.sqrt(mean_squared_error(y_test, global_avg_pred)))\n",
    "    maeglobalavg.append(mean_absolute_error(y_test, global_avg_pred))\n",
    "    \n",
    "    rmseuseravg.append(math.sqrt(mean_squared_error(y_test,user_avg_pred)))\n",
    "    maeuseravg.append(mean_absolute_error(y_test, user_avg_pred))\n",
    "    \n",
    "    rmseitemavg.append(math.sqrt(mean_squared_error(y_test, item_avg_pred)))\n",
    "    maeitemavg.append(mean_absolute_error(y_test, item_avg_pred))\n",
    "\n",
    "# print averages of lists\n",
    "print('global rmse: ', sum(rmseglobalavg)/len(rmseglobalavg))\n",
    "print('global mae: ', sum(maeglobalavg)/len(maeglobalavg))\n",
    "print('user rmse: ', sum(rmseuseravg)/len(rmseuseravg))\n",
    "print('user mae: ', sum(maeuseravg)/len(maeuseravg))\n",
    "print('item rmse: ', sum(rmseitemavg)/len(rmseitemavg))\n",
    "print('item mae: ', sum(maeitemavg)/len(maeitemavg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c88097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 rmse:  1.7776898530848342\n",
      "model 1 mae:  1.4458338227929346\n",
      "model 2 rmse:  1.114740675708551\n",
      "model 2 mae:  0.9301613262832701\n"
     ]
    }
   ],
   "source": [
    "# lists to put results in\n",
    "model1rmseavg = []\n",
    "model2rmseavg = []\n",
    "model1maeavg = []\n",
    "model2maeavg = []\n",
    "\n",
    "for trainid, testid in kf.split(ratings):\n",
    "    \n",
    "    X_train = ratings[['UserID','MovieID']].iloc[trainid]\n",
    "    X_test = ratings[['UserID','MovieID']].iloc[testid]\n",
    "    y_train = ratings['Rating'].iloc[trainid]\n",
    "    y_test = ratings['Rating'].iloc[testid]\n",
    "    \n",
    "    # predict using alpha*user + beta*item\n",
    "    model1 = LinearRegression(fit_intercept = False)\n",
    "    model1.fit(X_train, y_train)\n",
    "    pred1 = model1.predict(X_test)\n",
    "    \n",
    "    # predict using alpha*user + beta*item + gamma\n",
    "    model2 = LinearRegression()\n",
    "    model2.fit(X_train, y_train)\n",
    "    pred2 = model2.predict(X_test)\n",
    "    \n",
    "    # compute rmse and mae\n",
    "    rmse1 = math.sqrt(mean_squared_error(y_test, pred1))\n",
    "    mae1 = mean_absolute_error(y_test, pred1)\n",
    "    rmse2 = math.sqrt(mean_squared_error(y_test, pred2))\n",
    "    mae2 = mean_absolute_error(y_test, pred2)\n",
    "    \n",
    "    # add results to list\n",
    "    model1rmseavg.append(rmse1)\n",
    "    model2rmseavg.append(rmse2)\n",
    "    model1maeavg.append(mae1)\n",
    "    model2maeavg.append(mae2)\n",
    "    \n",
    "# print averages of lists\n",
    "print('model 1 rmse: ', sum(model1rmseavg)/len(model1rmseavg))\n",
    "print('model 1 mae: ', sum(model1maeavg)/len(model1maeavg))\n",
    "print('model 2 rmse: ', sum(model2rmseavg)/len(model2rmseavg))\n",
    "print('model 2 mae: ', sum(model2maeavg)/len(model2maeavg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af4632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
